[{"id":0,"href":"/posts/rust/rust-hello-cargo/","title":"Rust Hello Cargo","section":"Rust","content":"   document.addEventListener(\"DOMContentLoaded\", function() { renderMathInElement(document.body, { delimiters: [ {left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\(', right: '\\\\)', display: false}, {left: '\\\\[', right: '\\\\]', display: true} ], throwOnError : false }); });  Cargo plays a role as build system and package manager of Rust. The simplest Rust programs like hello-world do not have any depedencies on libraries and other files. However, the complex Rust programs has dependecies. In this case, Cargo helps Rustaceans to download, compile, distribute, and upload Rust packages for handling complicated dependencies. Cargo is a built-in pakage manager that comes with Rust installation. Below command will help you check the installation status and the Cargo version.\n\u0026gt; cargo --version   Output ↕  cargo 1.58.0    Create a Project using Cargo #  On any operating system, a Project can be created with Cargo.\n\u0026gt; cargo new [project name] If you create a project, named hello-cargo, you could check below output and find that \u0026lsquo;hello-cargo\u0026rsquo; directory is created.\n\u0026gt; cargo new hello-cargo   Output ↕  Created binary (application) hello-cargo package     mermaid.initialize({ \"flowchart\": { \"useMaxWidth\":true }, \"theme\": \"default\" } ) graph LR root[hello-cargo] -- 1[Cargo.toml] root -- 2[src] subgraph 2g[All project source files] 2 -- 21[main.rs] end subgraph 1g[Cargo manifest file] 1 end click root \"https://github.com/SungjaeJung1031/rust/tree/main/rust-hello-cargo/hello-cargo\" click 1 \"https://github.com/SungjaeJung1031/rust/blob/main/rust-hello-world/hello-world.rs\" click 2 \"https://github.com/SungjaeJung1031/rust/tree/main/rust-hello-cargo/hello-cargo/src\" click 21 \"https://github.com/SungjaeJung1031/rust/blob/main/rust-hello-cargo/hello-cargo/src/main.rs\" linkStyle 0 stroke-width:1px; style 1g fill:transparent,stroke:#E5E5E5,stroke-width:1px,stroke-dasharray:5; style 2g fill:transparent,stroke:#323232,stroke-width:1px,stroke-dasharray:5; Guide\nClick the items in above diagram to check the code example  The Cargo. toml is the manifest file for Cargo.\n0 1 2 3 4 5 6 7  [package] name = \u0026#34;hello-cargo\u0026#34; version = \u0026#34;0.1.0\u0026#34; edition = \u0026#34;2021\u0026#34; # See more keys and their definitions at https://doc.rust-lang.org/cargo/reference/manifest.html  [dependencies]   Cargo.toml look similar to the above code. Carog.toml is comparised of section with square brackets and key value pair (e.g. name = \u0026ldquo;hello-cargo\u0026rdquo;). [package] section is configuration of a package, and [depedencies] section describes the list of a project\u0026rsquo;s dependecies. Additional TOML documentation can be found on TOML website\nThen, how we could build the project using Cargo? For that lets first enter code in main.rs since the file is empty for the first time.\n0 1 2 3  fn main() { println!(\u0026#34;Hello, cargo!\u0026#34;); }   After saving main.rs, enter the following command in the project\u0026rsquo;s root directory to build the Cargo project.\n\u0026gt; cargo build   Output ↕  Compiling hello-cargo v0.1.0 (/Users/sungjaejung1031/dev/rust/rust-hello-cargo/hello-cargo) Finished dev [unoptimized + debuginfo] target(s) in 0.46s    Then Cargo.lock file and target directory will be created in the root directory.\nTBC\u0026hellip;\nSource Code\nSource code in this post is provided in the link.\n"},{"id":1,"href":"/posts/rust/rust-hello-world/","title":"Rust Hello World","section":"Rust","content":"   document.addEventListener(\"DOMContentLoaded\", function() { renderMathInElement(document.body, { delimiters: [ {left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\(', right: '\\\\)', display: false}, {left: '\\\\[', right: '\\\\]', display: true} ], throwOnError : false }); });   Create hello-world.rs file and enter the code in the below code block.  0 1 2 3  fn main() { println!(\u0026#34;Hello, world!\u0026#34;); }   Save the file and run the file  \u0026gt; rustc hello-world.rs \u0026gt; ./hello-world   Output ↕  Hello, world!    Source Code\nSource code in this post is provided in the link.\n"},{"id":2,"href":"/posts/ai/rl/rl-basic/","title":"RL Basic","section":"Reinforcement Learning","content":"   document.addEventListener(\"DOMContentLoaded\", function() { renderMathInElement(document.body, { delimiters: [ {left: '$$', right: '$$', display: true}, {left: '$', right: '$', display: false}, {left: '\\\\(', right: '\\\\)', display: false}, {left: '\\\\[', right: '\\\\]', display: true} ], throwOnError : false }); });  Machine learning has 3 basic areas: supervised learning, unsupervised learning, and reinforcement learning. Below is the brief comparison of the areas.\n   Area Supervised Learning Unsupervised Learning Reinforcement Learning     Definition Approach that uses labeled datasets to train algorithms that classifies data or predicts outcomes accurately Approach to analyze and cluster unlabeled data Approach that trains a software agent from trials and errors to behave rationally in an environment to maximize the notion of cumulative reward   Problem Classification / Regression Clustering / Association / Dimensionality reduction Exploitation / Exploration   Algorithm Linear- or Logistic- regression / SVM / KNN / Random forest / Neural networks / Naive Bayes K-Means / Apriori / Hierarchical clustering / SVD Q-Learning / SARSA    This post will explain core terminologies and equations to understand the various algorithms of reinforcement learning.\nIt would be helpful to have prior knowledge regarding mathematics, statistics, and machine learning for fully understanding this post. Below pages provide a wealth of information regarding the knowledge.\n  Math, Khan Academy  Statistics Online, PennState Eberly Collge of Science  Machine Learning Crash Course, Google          Fig 1. Reinforcement Learning Cycle    Fig 1 shows the key mechanism of the reinforcement learning (RL). An agent and an environment interact every step. The agent makes an action ($A_t$) with respect to the state ($S_t$) at a time step $t$. The environment generates a next state ($S_{t+1}$) for the next time step $t+1$ and the agent get the return (reward or penalty) ($R_{t+1}$). The goal of the RL is to make the agent find the best way of interaction with respect to the given policy and/or goal.\nLet\u0026rsquo;s take an example of a game scenario for describing the RL learning cycle and its mechanism.\n        Fig 2. Reinforcement Learning Cycle Example     There is a game whose objective is to move a game character from the start to the goal in a map with a boundary condition (or the wall) and the following rule.\n Reward: if the character finds the goal point. Reward: if a game is finished faster than the previous game. Penalty: if the character hits the boundary or wall in the map.  Here, the goal of the game is to maximize the reward, and the user will try to find the best way to get the highest reward.\nEvery time step, a user can control the game character to go to the next point, and there are four options; left, up, right, and down. Every move, the state of the character is changed since the place and surrounding environment of the character differ from the previous time step.\nThe core RL cycle in Fig 1** can be formulated as follows:\n$$ Q(s_{t}, a_{t}) ← Q(s_{t}, a_{t}) + \\alpha[r_{t+1} + \\gamma\\max_{a}Q(s_{t+1}, a) - Q(S_{t}, a_{t})] \\tag{1} $$\nThe following sections will describe the terms to understand the core RL Equation.\n  Agent  State and Observation  Action  Policy  Trajectory  Return  Exploration vs. Exploitation  Greedy Algorithm  RL Problem  Agent #  An agent is a component who acts in accordance with the Equation 1. The action of the agent is optimized by trials and errors. The obtimized action can achieve the goal to makes the maximum cumulative return (reward or penalty).\nState and Observation #  A state $s$ describes the world and an observation $o$ is fragmentary information of a state. Observations are the information that an agent can sense the state of the world. The terms fully observed or partially observed are used when the environment is completely and partially observed respectively. In the case of automobile vehicles, data from various sensors such as radar, lidar, camera, and wheel speed sensors can be examples of observation. The observations can be the velocity, acceleration, yaw rate, the distance between the neighboring cars, and the number of passengers in an ego vehicle. In automotive control systems, the agent checks the various given internal and external states of the vehicle through the observations. After that, the agent decides an action to make an optimal control the vehicle with respect to the state.\nAction #  An action $a$ is an agent\u0026rsquo;s method that allows to interact or update its environment, and transfer between states. An action space is defined as a set of all valid actions in a given environment. In the case of Fig 2, environment in the 4x4 map are given to the game character. In this case, the environment is called as a discrete action space where a finite number of actions are valid to the agent to move on to the next state. The opposite is continuous action space who has an infinite number of actions in a continuous domain for the next state.\nPolicy #  A policy is a rule of an agent to make an action in an action space. The policy tends to maximize return and there are two types: deterministic policy ($\\mu$) and stochastic policy ($\\pi$).\n$$ a_{t} = \\mu(s_t) $$ $$ a_{t} \\sim \\pi(\\cdot|s_{t}) $$\n$\\cdot$ of the stochastic policy means that all the possible adctions in a state $s$ at time $t$. The above equations for deterministic and stochastic policies are parameterized to be computable functions with respect to a set of parameters (weights and biases of a neural network) so that an optimization algorithm works for the policies. Parameters of deterministic and stochastic policies are denoted as $\\theta$ or $\\phi$.\n$$ a_{t} = \\mu_{\\theta}(s_t) $$ $$ a_{t} \\sim \\pi_{\\theta}(\\cdot|s_{t}) $$\nThere are two most common kinds of stochastic policies in the deep RL: categorial policies used in discrete action space and diagonal Gaussian policies used in continuous action space.\nWhile using and training stochastic policies, two computations are of importance:\n sampling actions from the policy computing log-likelihoods of particular actions, $log\\pi_{\\theta}(a|s)$.  The log-likelihoods of categorial policies with deterministic action samples $a$ is as follows,\n$$ \\begin{align} log\\pi_{\\theta}(a|s) =\\text{ }\u0026amp;log[P_{\\theta}(S)]_a , \\nonumber \\newline \\text{where } \u0026amp; P_{\\theta}(s) \\text{ is the last layer of probabilities}, \\nonumber \\end{align} $$\nand the log-likelihoods of diagonal Gaussian policies with stochastic samples $a$ is as follows,\n$$ \\begin{align} \\log\\pi_{\\theta}(a|s) =\\text{ }\u0026amp;-\\frac{1}{2} (\\sum^{k}_{i=1}(\\frac{(a_i-\\mu_i)^2}{\\sigma^2_i})+k\\log2\\pi) \\nonumber \\\\ \\text{where } \u0026amp; a \\text{ is } \\mathcal{k}\\text{-dimensional action} \\nonumber \\\\ \u0026amp; \\mu \\text{ is mean} \\nonumber \\\\ \u0026amp; \\sigma \\text{ is standard deviation} \\nonumber \\\\ \\end{align} $$\n$$ \\begin{align} a =\\text{ }\u0026amp; \\mu_{\\theta}(s) + \\sigma_{theta}(s) \\odot z, \\nonumber \\\\ \\text{where } \u0026amp; \\mu_{\\theta}(s) \\text{ is mean action} \\nonumber \\\\ \u0026amp; \\sigma_{theta}(s) \\text{ is standard deviation} \\nonumber \\\\ \u0026amp; z \\text{ is noise from a spherical Gaussian, } z \\sim \\mathcal{N}(0, I) \\nonumber \\end{align} $$\nTrajectory #  A trajectory $\\tau$ is a sequence of states and actions in the world,\n$$ \\tau = (s_0, a_0, s_1, a_1,\u0026hellip;) $$\n$s_0$ is the first state of the world and randomly sampled from the start-state-distribution, sometimes denoted by $\\rho_0(\\cdot)$:\n$$ s_0 \\sim \\rho_0(\\cdot) $$\nState transitions (what happens to the world between the state at time $t$, $s_t$, and the state at time $t+1$, $s_{t+1}$), are governed by the natural laws of the environment, and depend only on the most recent action, $a_t$. They can be either deterministic,\n$$ s_{t+1} = f(s_t, a_t) $$\nor stochastic,\n$$ s_{t+1} \\sim P(\\cdot|s_t, a_t). $$\nAction at every step comes from an agent in accordance with its policy.\nFig 2 can be used to describe trajectory in RL, and the trajectory can be graphically presented as the right side of Fig 3.\n        Fig 3. Reinforcement Learning Trajectory Example    Return #  A return (or reward) $r$ is generated by the return (or reward) function $R$ in RL. A reward at a specific time $t$ may differ with respect to the dependence with the current state of the world, the action at the same time, and the next state of the world: $$ r_t = R(s_t, a_t, s_{t+1}) \\text{ or } R(s_t, a_t) \\text { or } R(s_t) $$\nIn RL, the agent tries to maximize cumulative reward over a trajectory. The cumulative reward is denoted as $R(\\tau)$ and accumulated in $T$-step trajectory, finite time domain of the world.\n$$ R(\\tau) = \\sum_{t=0}^T r_{t} $$\nThe above equation of the return at each time step is valued based on the return of the next step. Such return is the finite-horizon undiscounted return. If the return is undervalued with respect to the time passed from the current time step. Above equation can be refomulated with the discount factor $\\gamma \\in (0,1)$ and the refomulated equation is the finite-horizon discounted return.\n$$ R(\\tau) = \\sum_{t=0}^T \\gamma^{t} r_{t} $$\nThe return can be from the infinite time domain of the world, and in that case, the equation can be formulated as the infinite-horizon discounted return.\n$$ R(\\tau) = \\sum_{t=0}^\\infty \\gamma^{t} r_{t} $$\nThe reason why RL uses only the infinite form with discounted return is that mathematically, an infinite sum of rewards with the discounted factor is always converged to a finite value as various types of infinite series so that the return value can be finite and can be compared with other returns in the different trajectories.\nExploration vs. Exploitation #  Before moving on to expoloration and exploitation in RL, let\u0026rsquo;s bring back the equation \\eqref{rl-core} here, together with RL terminologies.\n$$ \\begin{align} Q(s_{t}, a_{t}) \u0026amp; ← Q(s_{t}, a_{t}) + \\alpha[r_{t+1} + \\gamma\\max_{a}Q(s_{t+1}, a) - Q(s_{t}, a_{t})] \\nonumber \\\\ \u0026amp; ← (1-\\alpha)Q(s_{t}, a_{t}) + \\alpha[r_{t+1} + \\gamma\\max_{a}Q(s_{t+1}, a)] \\nonumber \\end{align} $$\n$$ \\begin{align} \\text{where } \u0026amp; Q(s_t, a_t) \\text{ is the Q or Q-function at time } t, \\nonumber \\\\ \u0026amp; t \\text{ is the time step} \\nonumber \\\\ \u0026amp; a \\text{ is the action} \\nonumber \\\\ \u0026amp; s \\text{ is the state} \\nonumber \\\\ \u0026amp; \\alpha \\text{ is the learning rate} \\nonumber \\\\ \u0026amp; r \\text{ is the return} \\nonumber \\\\ \u0026amp; \\gamma \\text{ is the discount factor} \\nonumber \\end{align} $$\nFrom this section, we have seen almost all the definitions except the Q-function $Q(s_t, a_t)$ and the learning rate $\\alpha$.\nThe Q-function refers that the RL algorithm updates, and it is the expected reward with respect to an action $a$ given state $s$ at time $t$ in RL.\nIn the equation \\eqref{rl-core}, $Q(s_t, a_t)$ is the old value of Q-function and $\\gamma\\max_{a}Q(s_{t+1}, a)$ is the estimation of the optimal future value of Q-function. The core RL algorithm updates the old value from the right hand side of the equation that adds the temporal difference $r_{t+1} + \\gamma\\max_{a}Q(s_{t+1}, a) - Q(s_{t}, a_{t})$ to the old value $Q(s_{t}, a_{t})$ with learning rate.\nThe temporal difference means the difference between the old value $Q(s_{t}, a_{t})$ and new value or temporal difference target $r_{t+1} + \\gamma\\max_{a}Q(s_{t+1}, a)$.\nThe learning rate means how much a Q-function in the current state the agent considers against that in the next state.\nNow, we know all the terms of the RL core equation. Then, let\u0026rsquo;s take a look how RL works in accordance with the equation.\nLet\u0026rsquo;s suppose that we are playing the RL cycle example game. The game will be played 6 times successively.\nThe game uses stochastic policy $\\pi$, but very luckily, the agent gets the states from the same probabilistic actions every time: (0,0)-\u0026gt;(0,1)-\u0026gt;(0,2)-\u0026gt;(0,3)-\u0026gt;(1,2)-\u0026gt;(2,2)-\u0026gt;(2,1)-\u0026gt;(goal).\nThe very first game scenario can be visualized as below.\n        Fig 4. Reinforcement Learning Example, Scenario #1    The right side of the Fig 4 is a Q-Table that is the representation of the values of Q-functions. The Q-Table is initialized to 0 at the begining. After all the way through the journey of the character, only the Q-function, $Q(s_5, a_5)$ with action left given the state (2,1), is updated since only the destination state, goal point, awards the 2 points.\nQ-functions with an action awards the best expected return given the prior stateㄴ will be updated successively after 5 times additional games as below figures.\n Then let\u0026rsquo;s talk about the exploration and exploitation.\nThe exploration that searches all neighboring states of the current state. In other words, the exploration searches over the whole sample spaces of an action with high variance.\nAs opposed to that, exploitation means that searching fewer regions to improve the quality with a small perturbation.\nAfter 6 times successive game in the example, the character finds a path to the goal. With the high-inclination to exploitation, the character will follow the known path. Vice versa, the exploration leads the character to search other paths to the goal. Actually, in the example, there is a shorter path to the goal: (0,0)-\u0026gt;(1,0)-\u0026gt;(2,0)-\u0026gt;(2,1)-\u0026gt;(goal). Therefore, in this case, the RL algorithm should incline to exploration to find the better path to the goal.\nGreedy Algorithm #  $$ \\begin{align} Q(s_{t}, a_{t}) \u0026amp; ← Q(s_{t}, a_{t}) + \\alpha[r_{t+1} + \\gamma\\max_{a}Q(s_{t+1}, a) - Q(s_{t}, a_{t})] \\nonumber \\\\ \u0026amp; ← (1-\\alpha)Q(s_{t}, a_{t}) + \\alpha[r_{t+1} + \\gamma\\max_{a}Q(s_{t+1}, a)] \\nonumber \\end{align} $$\nThe RL core algorithm finds only one optimal action that maximizes the reward in the next state due to the term, $\\max_{a}Q(s_{t+1}, a)$. Therefore, other actions is not considered. Such selection algorthm is called as greedy algorithm. With the greedy algorithm, RL core algorithm inclines to exploitation.\nThen, what makes RL use exploration? The answer is $\\epsilon$-greedy algorithm.\n$$ A \\leftarrow \\biggl \\{\\begin{matrix} max_{a}Q(s_{t+1}, a) \u0026amp;\u0026amp; \\text{with probability 1-}\\epsilon \\\\ \\text{a random action} \u0026amp;\u0026amp; \\text{with probability }\\epsilon \\end{matrix}\\biggr. $$\n$\\epsilon$-greedy algorithm selects the optimal action that gives the maximized return for a proportion 1-$\\epsilon$. A random action is chosen for a proportion $\\epsilon$. However, if the RL algorithm is learning by using $\\epsilon$-greedy algorithm after exploring diverse paths with high variation, it will become inefficient. Therefore, decaying $\\epsilon$-greedy algrithm is used.\nThe equation \\eqref{rl-core} can be replaced as shown above. $\\epsilon \\in (0,1)$ in $\\epsilon$-greedy algorithm is represented $\\alpha$ in RL core algorithm.\nRL Problem #  No matter what return type and policy are used, an agent tries to maximize the return. As seen in the above examples, there are various trajectory to get the return, and RL learns with trial and error, and updates the Q-Table while finding the path with the maximal expected return. This can be represented as formular and let\u0026rsquo;s take a look step by step.\nState transitions and the policy are stochastic with exploration based learning. The probability of $T$-step trajectory can be represented as below:\n$$ \\begin{align} P(\\tau|\\pi) =\\text{ }\u0026amp; \\rho_0 (s_0) \\prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \\pi(a_t | s_t). \\nonumber \\\\ \\text{where } \u0026amp; \\tau \\text{ is a sequence of states and action in the environment} \\nonumber \\\\ \u0026amp; \\rho_0 (s_0) \\text{ is the start-state distribution } \\nonumber \\\\ \u0026amp; P(s_{t+1} | s_t, a_t) \\text{ is the state transition} \\nonumber \\\\ \u0026amp; s \\text{ is the state} \\nonumber \\\\ \u0026amp; \\pi(a_t | s_t) \\text{ is the policy} \\nonumber \\end{align} $$\nSimilary to expected value in probability theory, the expected return $J(\\pi)$ (for whichever measure) can be formulated as following.\n$$ J(\\pi) = \\int_{\\tau} P(\\tau|\\pi) R(\\tau) = E_{\\tau \\sim \\pi} [R(\\tau)] $$\nFinally, the central optimization problem in RL is the optimal policy $\\pi^*$\n$$ \\pi^* = \\arg \\max_{\\pi} J(\\pi) $$\n"}]